{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9905418-5d67-47f1-b434-c29ec57246cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.common.exceptions import TimeoutException, NoSuchElementException\n",
    "import time\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7427d446-81ac-4c19-8fcb-83aff96bf4be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct file path\n",
    "file_path = r\"C:\\Users\\Sukkiiii\\Desktop\\CAPSTONE PROJECT\\RED BUS\\df_k.csv\"\n",
    "\n",
    "# Reading the CSV file\n",
    "df = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "084b943d-50bb-46ca-ba7b-7644a6ecf7c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping Successfully Completed\n"
     ]
    }
   ],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.action_chains import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver_k = webdriver.Chrome()\n",
    "\n",
    "# Data storage lists\n",
    "Bus_names_k = []\n",
    "Bus_types_k = []\n",
    "Start_Time_k = []\n",
    "End_Time_k = []\n",
    "Ratings_k = []\n",
    "Total_Duration_k = []\n",
    "Prices_k = []\n",
    "Seats_Available_k = []\n",
    "Route_names = []\n",
    "Route_links = []\n",
    "\n",
    "# Loop through DataFrame rows\n",
    "for i, r in df.iterrows():\n",
    "    link = r[\"ROUTE_link\"]\n",
    "    routes = r[\"Route_name\"]\n",
    "\n",
    "    driver_k.get(link)\n",
    "    time.sleep(2)\n",
    "\n",
    "    # Click all elements with specific links\n",
    "    elements = driver_k.find_elements(By.XPATH, f\"//a[contains(@href, '{link}')]\")\n",
    "    for element in elements:\n",
    "        element.click()\n",
    "        time.sleep(2)\n",
    "\n",
    "    try:\n",
    "        clicks = driver_k.find_elements(By.XPATH, \"//div[@class='button']\")\n",
    "        for click in clicks:\n",
    "            click.click()\n",
    "    except Exception as e:\n",
    "        print(f\"Error clicking: {e}\")\n",
    "        continue\n",
    "\n",
    "    # Scroll through the page\n",
    "    scrolling = True\n",
    "    while scrolling:\n",
    "        old_page_source = driver_k.page_source\n",
    "        ActionChains(driver_k).send_keys(Keys.PAGE_DOWN).perform()\n",
    "        time.sleep(5)\n",
    "        new_page_source = driver_k.page_source\n",
    "        if new_page_source == old_page_source:\n",
    "            scrolling = False\n",
    "\n",
    "    # Extract data\n",
    "    bus_name = driver_k.find_elements(By.XPATH, \"//div[@class='travels lh-24 f-bold d-color']\")\n",
    "    bus_type = driver_k.find_elements(By.XPATH, \"//div[@class='bus-type f-12 m-top-16 l-color evBus']\")\n",
    "    start_time = driver_k.find_elements(By.XPATH, \"//*[@class='dp-time f-19 d-color f-bold']\")\n",
    "    end_time = driver_k.find_elements(By.XPATH, \"//*[@class='bp-time f-19 d-color disp-Inline']\")\n",
    "    total_duration = driver_k.find_elements(By.XPATH, \"//*[@class='dur l-color lh-24']\")\n",
    "    rating = driver_k.find_elements(By.XPATH, \"//div[@class='clearfix row-one']/div[@class='column-six p-right-10 w-10 fl']\")\n",
    "    price = driver_k.find_elements(By.XPATH, \"//*[@class='fare d-block']\")\n",
    "    seats = driver_k.find_elements(By.XPATH, \"//div[contains(@class, 'seat-left')]\")\n",
    "\n",
    "    # Append data to respective lists\n",
    "    for bus in bus_name:\n",
    "        Bus_names_k.append(bus.text)\n",
    "        Route_links.append(link)\n",
    "        Route_names.append(routes)\n",
    "    for bus_type_elem in bus_type:\n",
    "        Bus_types_k.append(bus_type_elem.text)\n",
    "    for start_time_elem in start_time:\n",
    "        Start_Time_k.append(start_time_elem.text)\n",
    "    for end_time_elem in end_time:\n",
    "        End_Time_k.append(end_time_elem.text)\n",
    "    for total_duration_elem in total_duration:\n",
    "        Total_Duration_k.append(total_duration_elem.text)\n",
    "    for rating_elem in rating:\n",
    "        Ratings_k.append(rating_elem.text)\n",
    "    for price_elem in price:\n",
    "        Prices_k.append(price_elem.text)\n",
    "    for seats_elem in seats:\n",
    "        Seats_Available_k.append(seats_elem.text)\n",
    "\n",
    "print(\"Scraping Successfully Completed\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f1fb1a53-9900-4980-a143-ba1de411e83a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of data lists: {'Bus_name': 187, 'Bus_type': 187, 'Start_time': 187, 'End_time': 187, 'Total_duration': 187, 'Price': 187, 'Seats_Available': 187, 'Ratings': 187, 'Route_link': 187, 'Route_name': 187}\n",
      "Data successfully saved to C:/Users/Sukkiiii/Desktop/CAPSTONE PROJECT/RED BUS/df_buses_1.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create DataFrame\n",
    "data = {\n",
    "    'Bus_name': Bus_names_k,\n",
    "    'Bus_type': Bus_types_k,\n",
    "    'Start_time': Start_Time_k,\n",
    "    'End_time': End_Time_k,\n",
    "    'Total_duration': Total_Duration_k,\n",
    "    'Price': Prices_k,\n",
    "    \"Seats_Available\": Seats_Available_k,\n",
    "    \"Ratings\": Ratings_k,\n",
    "    'Route_link': Route_links,\n",
    "    'Route_name': Route_names\n",
    "}\n",
    "\n",
    "# Validate lengths of lists\n",
    "lengths = {key: len(value) for key, value in data.items()}\n",
    "print(\"Lengths of data lists:\", lengths)\n",
    "\n",
    "# Check for inconsistencies\n",
    "if len(set(lengths.values())) != 1:\n",
    "    print(\"Error: Mismatch in lengths of data lists.\")\n",
    "else:\n",
    "    # Save to CSV\n",
    "    path = r\"C:/Users/Sukkiiii/Desktop/CAPSTONE PROJECT/RED BUS/df_buses_1.csv\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)  # Create directories if they don't exist\n",
    "    df_buses_1 = pd.DataFrame(data)\n",
    "    df_buses_1.to_csv(path, index=False)\n",
    "    print(f\"Data successfully saved to {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79fcfc64-ec70-47f9-bb2f-5ca3d5eeb305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct file path\n",
    "file_path = r\"C:\\Users\\Sukkiiii\\Desktop\\CAPSTONE PROJECT\\RED BUS\\df_a.csv\"\n",
    "\n",
    "# Reading the CSV file\n",
    "df_1= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3332bce4-d900-42d8-a157-c8c44f87b13f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data lengths: 571, 571, 571, 571\n",
      "Scraping Successfully Completed\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver_A = webdriver.Chrome()\n",
    "\n",
    "# Data storage lists\n",
    "Bus_names_a = []\n",
    "Bus_types_a = []\n",
    "Start_Time_a = []\n",
    "End_Time_a = []\n",
    "Ratings_a = []\n",
    "Total_Duration_a = []\n",
    "Prices_a = []\n",
    "Seats_Available_a = []\n",
    "Route_names = []\n",
    "Route_links = []\n",
    "\n",
    "# Loop through DataFrame rows\n",
    "for i, r in df_1.iterrows():\n",
    "    link = r[\"ROUTE_link\"]\n",
    "    routes = r[\"Route_name\"]\n",
    "\n",
    "    try:\n",
    "        # Open link\n",
    "        driver_A.get(link)\n",
    "        WebDriverWait(driver_A, 10).until(EC.presence_of_element_located((By.XPATH, \"//body\")))\n",
    "\n",
    "        # Click elements (if required)\n",
    "        elements = driver_A.find_elements(By.XPATH, f\"//a[contains(@href, '{link}')]\")\n",
    "        for element in elements:\n",
    "            element.click()\n",
    "            WebDriverWait(driver_A, 10).until(EC.presence_of_element_located((By.XPATH, \"//body\")))\n",
    "\n",
    "        # Scroll through the page\n",
    "        scrolling = True\n",
    "        while scrolling:\n",
    "            old_page_source = driver_A.page_source\n",
    "            ActionChains(driver_A).send_keys(Keys.PAGE_DOWN).perform()\n",
    "            time.sleep(2)  # Adjust as needed\n",
    "            new_page_source = driver_A.page_source\n",
    "            if new_page_source == old_page_source:\n",
    "                scrolling = False\n",
    "\n",
    "        # Extract data\n",
    "        bus_name = driver_A.find_elements(By.XPATH, \"//div[@class='travels lh-24 f-bold d-color']\")\n",
    "        bus_type = driver_A.find_elements(By.XPATH, \"//div[@class='bus-type f-12 m-top-16 l-color evBus']\")\n",
    "        start_time = driver_A.find_elements(By.XPATH, \"//*[@class='dp-time f-19 d-color f-bold']\")\n",
    "        end_time = driver_A.find_elements(By.XPATH, \"//*[@class='bp-time f-19 d-color disp-Inline']\")\n",
    "        total_duration = driver_A.find_elements(By.XPATH, \"//*[@class='dur l-color lh-24']\")\n",
    "        rating = driver_A.find_elements(By.XPATH, \"//div[@class='clearfix row-one']/div[@class='column-six p-right-10 w-10 fl']\")\n",
    "        price = driver_A.find_elements(By.XPATH, \"//*[@class='fare d-block']\")\n",
    "        seats = driver_A.find_elements(By.XPATH, \"//div[contains(@class, 'seat-left')]\")\n",
    "\n",
    "        # Append data to respective lists\n",
    "        for bus in bus_name:\n",
    "            Bus_names_a.append(bus.text)\n",
    "            Route_links.append(link)\n",
    "            Route_names.append(routes)\n",
    "        for bus_type_elem in bus_type:\n",
    "            Bus_types_a.append(bus_type_elem.text)\n",
    "        for start_time_elem in start_time:\n",
    "            Start_Time_a.append(start_time_elem.text)\n",
    "        for end_time_elem in end_time:\n",
    "            End_Time_a.append(end_time_elem.text)\n",
    "        for total_duration_elem in total_duration:\n",
    "            Total_Duration_a.append(total_duration_elem.text)\n",
    "        for rating_elem in rating:\n",
    "            Ratings_a.append(rating_elem.text)\n",
    "        for price_elem in price:\n",
    "            Prices_a.append(price_elem.text)\n",
    "        for seats_elem in seats:\n",
    "            Seats_Available_a.append(seats_elem.text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Check data consistency\n",
    "print(f\"Scraped data lengths: {len(Bus_names_a)}, {len(Bus_types_a)}, {len(Start_Time_a)}, {len(End_Time_a)}\")\n",
    "\n",
    "print(\"Scraping Successfully Completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6908a38c-3573-408f-9687-16f41d9c3f22",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of data lists: {'Bus_name': 571, 'Bus_type': 571, 'Start_time': 571, 'End_time': 571, 'Total_duration': 571, 'Price': 571, 'Seats_Available': 571, 'Ratings': 571, 'Route_link': 571, 'Route_name': 571}\n",
      "Data successfully saved to C:/Users/Sukkiiii/Desktop/CAPSTONE PROJECT/RED BUS/df_buses_2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create DataFrame\n",
    "data_1= {\n",
    "    'Bus_name': Bus_names_a,\n",
    "    'Bus_type': Bus_types_a,\n",
    "    'Start_time': Start_Time_a,\n",
    "    'End_time': End_Time_a,\n",
    "    'Total_duration': Total_Duration_a,\n",
    "    'Price': Prices_a,\n",
    "    \"Seats_Available\": Seats_Available_a,\n",
    "    \"Ratings\": Ratings_a,\n",
    "    'Route_link': Route_links,\n",
    "    'Route_name': Route_names\n",
    "}\n",
    "\n",
    "# Validate lengths of lists\n",
    "lengths = {key: len(value) for key, value in data_1.items()}\n",
    "print(\"Lengths of data lists:\", lengths)\n",
    "\n",
    "# Check for inconsistencies\n",
    "if len(set(lengths.values())) != 1:\n",
    "    print(\"Error: Mismatch in lengths of data lists.\")\n",
    "else:\n",
    "    # Save to CSV\n",
    "    path = r\"C:/Users/Sukkiiii/Desktop/CAPSTONE PROJECT/RED BUS/df_buses_2.csv\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)  # Create directories if they don't exist\n",
    "    df_buses_2 = pd.DataFrame(data_1)\n",
    "    df_buses_2.to_csv(path, index=False)\n",
    "    print(f\"Data successfully saved to {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "91eab3ad-4efe-4f2c-b1bb-b894e14a04e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\Sukkiiii\\Desktop\\CAPSTONE PROJECT\\RED BUS\\df_t.csv\"\n",
    "\n",
    "# Reading the CSV file\n",
    "df_2= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ebb006b4-4663-4eda-b3d9-1dc8a571aeb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data lengths: 207, 207, 207, 207\n",
      "Scraping Successfully Completed\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver_T = webdriver.Chrome()\n",
    "\n",
    "# Data storage lists\n",
    "Bus_names_t = []\n",
    "Bus_types_t = []\n",
    "Start_Time_t = []\n",
    "End_Time_t = []\n",
    "Ratings_t = []\n",
    "Total_Duration_t = []\n",
    "Prices_t = []\n",
    "Seats_Available_t = []\n",
    "Route_names = []\n",
    "Route_links = []\n",
    "\n",
    "# Loop through DataFrame rows\n",
    "for i, r in df_2.iterrows():\n",
    "    link = r[\"ROUTE_link\"]\n",
    "    routes = r[\"Route_name\"]\n",
    "\n",
    "    try:\n",
    "        # Open link\n",
    "        driver_T.get(link)\n",
    "        WebDriverWait(driver_T, 10).until(EC.presence_of_element_located((By.XPATH, \"//body\")))\n",
    "\n",
    "        # Click elements (if required)\n",
    "        elements = driver_T.find_elements(By.XPATH, f\"//a[contains(@href, '{link}')]\")\n",
    "        for element in elements:\n",
    "            element.click()\n",
    "            WebDriverWait(driver_T, 10).until(EC.presence_of_element_located((By.XPATH, \"//body\")))\n",
    "\n",
    "        # Scroll through the page\n",
    "        scrolling = True\n",
    "        while scrolling:\n",
    "            old_page_source = driver_T.page_source\n",
    "            ActionChains(driver_T).send_keys(Keys.PAGE_DOWN).perform()\n",
    "            time.sleep(2)  # Adjust as needed\n",
    "            new_page_source = driver_T.page_source\n",
    "            if new_page_source == old_page_source:\n",
    "                scrolling = False\n",
    "\n",
    "        # Extract data\n",
    "        bus_name = driver_T.find_elements(By.XPATH, \"//div[@class='travels lh-24 f-bold d-color']\")\n",
    "        bus_type = driver_T.find_elements(By.XPATH, \"//div[@class='bus-type f-12 m-top-16 l-color evBus']\")\n",
    "        start_time = driver_T.find_elements(By.XPATH, \"//*[@class='dp-time f-19 d-color f-bold']\")\n",
    "        end_time = driver_T.find_elements(By.XPATH, \"//*[@class='bp-time f-19 d-color disp-Inline']\")\n",
    "        total_duration = driver_T.find_elements(By.XPATH, \"//*[@class='dur l-color lh-24']\")\n",
    "        rating = driver_T.find_elements(By.XPATH, \"//div[@class='clearfix row-one']/div[@class='column-six p-right-10 w-10 fl']\")\n",
    "        price = driver_T.find_elements(By.XPATH, \"//*[@class='fare d-block']\")\n",
    "        seats = driver_T.find_elements(By.XPATH, \"//div[contains(@class, 'seat-left')]\")\n",
    "\n",
    "        # Append data to respective lists\n",
    "        for bus in bus_name:\n",
    "            Bus_names_t.append(bus.text)\n",
    "            Route_links.append(link)\n",
    "            Route_names.append(routes)\n",
    "        for bus_type_elem in bus_type:\n",
    "            Bus_types_t.append(bus_type_elem.text)\n",
    "        for start_time_elem in start_time:\n",
    "            Start_Time_t.append(start_time_elem.text)\n",
    "        for end_time_elem in end_time:\n",
    "            End_Time_t.append(end_time_elem.text)\n",
    "        for total_duration_elem in total_duration:\n",
    "            Total_Duration_t.append(total_duration_elem.text)\n",
    "        for rating_elem in rating:\n",
    "            Ratings_t.append(rating_elem.text)\n",
    "        for price_elem in price:\n",
    "            Prices_t.append(price_elem.text)\n",
    "        for seats_elem in seats:\n",
    "            Seats_Available_t.append(seats_elem.text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Check data consistency\n",
    "print(f\"Scraped data lengths: {len(Bus_names_t)}, {len(Bus_types_t)}, {len(Start_Time_t)}, {len(End_Time_t)}\")\n",
    "\n",
    "print(\"Scraping Successfully Completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f8c9db1-8822-42b4-b605-427f8122b50c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of data lists: {'Bus_name': 207, 'Bus_type': 207, 'Start_time': 207, 'End_time': 207, 'Total_duration': 207, 'Price': 207, 'Seats_Available': 207, 'Ratings': 207, 'Route_link': 207, 'Route_name': 207}\n",
      "Data successfully saved to C:/Users/Sukkiiii/Desktop/CAPSTONE PROJECT/RED BUS/df_buses_3.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create DataFrame\n",
    "data_2= {\n",
    "    'Bus_name': Bus_names_t,\n",
    "    'Bus_type': Bus_types_t,\n",
    "    'Start_time': Start_Time_t,\n",
    "    'End_time': End_Time_t,\n",
    "    'Total_duration': Total_Duration_t,\n",
    "    'Price': Prices_t,\n",
    "    \"Seats_Available\": Seats_Available_t,\n",
    "    \"Ratings\": Ratings_t,\n",
    "    'Route_link': Route_links,\n",
    "    'Route_name': Route_names\n",
    "}\n",
    "\n",
    "# Validate lengths of lists\n",
    "lengths = {key: len(value) for key, value in data_2.items()}\n",
    "print(\"Lengths of data lists:\", lengths)\n",
    "\n",
    "# Check for inconsistencies\n",
    "if len(set(lengths.values())) != 1:\n",
    "    print(\"Error: Mismatch in lengths of data lists.\")\n",
    "else:\n",
    "    # Save to CSV\n",
    "    path = r\"C:/Users/Sukkiiii/Desktop/CAPSTONE PROJECT/RED BUS/df_buses_3.csv\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)  # Create directories if they don't exist\n",
    "    df_buses_3 = pd.DataFrame(data_2)\n",
    "    df_buses_3.to_csv(path, index=False)\n",
    "    print(f\"Data successfully saved to {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f302e66-2c56-4657-a994-5dba1fd3b5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\Sukkiiii\\Desktop\\CAPSTONE PROJECT\\RED BUS\\df_kt.csv\"\n",
    "\n",
    "# Reading the CSV file\n",
    "df_3= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dbfb91a5-b94b-4d6c-a65e-108c0909c3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data lengths: 137, 137, 137, 137\n",
      "Scraping Successfully Completed\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver_KT = webdriver.Chrome()\n",
    "\n",
    "# Data storage lists\n",
    "Bus_names_kt = []\n",
    "Bus_types_kt = []\n",
    "Start_Time_kt = []\n",
    "End_Time_kt = []\n",
    "Ratings_kt = []\n",
    "Total_Duration_kt = []\n",
    "Prices_kt = []\n",
    "Seats_Available_kt = []\n",
    "Route_names = []\n",
    "Route_links = []\n",
    "\n",
    "# Loop through DataFrame rows\n",
    "for i, r in df_3.iterrows():\n",
    "    link = r[\"ROUTE_link\"]\n",
    "    routes = r[\"Route_name\"]\n",
    "\n",
    "    try:\n",
    "        # Open link\n",
    "        driver_KT.get(link)\n",
    "        WebDriverWait(driver_KT, 10).until(EC.presence_of_element_located((By.XPATH, \"//body\")))\n",
    "\n",
    "        # Click elements (if required)\n",
    "        elements = driver_KT.find_elements(By.XPATH, f\"//a[contains(@href, '{link}')]\")\n",
    "        for element in elements:\n",
    "            element.click()\n",
    "            WebDriverWait(driver_KT, 10).until(EC.presence_of_element_located((By.XPATH, \"//body\")))\n",
    "\n",
    "        # Scroll through the page\n",
    "        scrolling = True\n",
    "        while scrolling:\n",
    "            old_page_source = driver_KT.page_source\n",
    "            ActionChains(driver_KT).send_keys(Keys.PAGE_DOWN).perform()\n",
    "            time.sleep(2)  # Adjust as needed\n",
    "            new_page_source = driver_KT.page_source\n",
    "            if new_page_source == old_page_source:\n",
    "                scrolling = False\n",
    "\n",
    "        # Extract data\n",
    "        bus_name = driver_KT.find_elements(By.XPATH, \"//div[@class='travels lh-24 f-bold d-color']\")\n",
    "        bus_type = driver_KT.find_elements(By.XPATH, \"//div[@class='bus-type f-12 m-top-16 l-color evBus']\")\n",
    "        start_time = driver_KT.find_elements(By.XPATH, \"//*[@class='dp-time f-19 d-color f-bold']\")\n",
    "        end_time = driver_KT.find_elements(By.XPATH, \"//*[@class='bp-time f-19 d-color disp-Inline']\")\n",
    "        total_duration = driver_KT.find_elements(By.XPATH, \"//*[@class='dur l-color lh-24']\")\n",
    "        rating = driver_KT.find_elements(By.XPATH, \"//div[@class='clearfix row-one']/div[@class='column-six p-right-10 w-10 fl']\")\n",
    "        price = driver_KT.find_elements(By.XPATH, \"//*[@class='fare d-block']\")\n",
    "        seats = driver_KT.find_elements(By.XPATH, \"//div[contains(@class, 'seat-left')]\")\n",
    "\n",
    "        # Append data to respective lists\n",
    "        for bus in bus_name:\n",
    "            Bus_names_kt.append(bus.text)\n",
    "            Route_links.append(link)\n",
    "            Route_names.append(routes)\n",
    "        for bus_type_elem in bus_type:\n",
    "            Bus_types_kt.append(bus_type_elem.text)\n",
    "        for start_time_elem in start_time:\n",
    "            Start_Time_kt.append(start_time_elem.text)\n",
    "        for end_time_elem in end_time:\n",
    "            End_Time_kt.append(end_time_elem.text)\n",
    "        for total_duration_elem in total_duration:\n",
    "            Total_Duration_kt.append(total_duration_elem.text)\n",
    "        for rating_elem in rating:\n",
    "            Ratings_kt.append(rating_elem.text)\n",
    "        for price_elem in price:\n",
    "            Prices_kt.append(price_elem.text)\n",
    "        for seats_elem in seats:\n",
    "            Seats_Available_kt.append(seats_elem.text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Check data consistency\n",
    "print(f\"Scraped data lengths: {len(Bus_names_kt)}, {len(Bus_types_kt)}, {len(Start_Time_kt)}, {len(End_Time_kt)}\")\n",
    "\n",
    "print(\"Scraping Successfully Completed\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "702c44f2-b4db-4c84-b84f-da369f8f1fd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of data lists: {'Bus_name': 137, 'Bus_type': 137, 'Start_time': 137, 'End_time': 137, 'Total_duration': 137, 'Price': 137, 'Seats_Available': 137, 'Ratings': 137, 'Route_link': 137, 'Route_name': 137}\n",
      "Data successfully saved to C:/Users/Sukkiiii/Desktop/CAPSTONE PROJECT/RED BUS/df_buses_4.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create DataFrame\n",
    "data_3= {\n",
    "    'Bus_name': Bus_names_kt,\n",
    "    'Bus_type': Bus_types_kt,\n",
    "    'Start_time': Start_Time_kt,\n",
    "    'End_time': End_Time_kt,\n",
    "    'Total_duration': Total_Duration_kt,\n",
    "    'Price': Prices_kt,\n",
    "    \"Seats_Available\": Seats_Available_kt,\n",
    "    \"Ratings\": Ratings_kt,\n",
    "    'Route_link': Route_links,\n",
    "    'Route_name': Route_names\n",
    "}\n",
    "\n",
    "# Validate lengths of lists\n",
    "lengths = {key: len(value) for key, value in data_3.items()}\n",
    "print(\"Lengths of data lists:\", lengths)\n",
    "\n",
    "# Check for inconsistencies\n",
    "if len(set(lengths.values())) != 1:\n",
    "    print(\"Error: Mismatch in lengths of data lists.\")\n",
    "else:\n",
    "    # Save to CSV\n",
    "    path = r\"C:/Users/Sukkiiii/Desktop/CAPSTONE PROJECT/RED BUS/df_buses_4.csv\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)  # Create directories if they don't exist\n",
    "    df_buses_4 = pd.DataFrame(data_3)\n",
    "    df_buses_4.to_csv(path, index=False)\n",
    "    print(f\"Data successfully saved to {path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "11e6f9c4-696a-4abf-bbb2-3536cfed3a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\Sukkiiii\\Desktop\\CAPSTONE PROJECT\\RED BUS\\df_r.csv\"\n",
    "\n",
    "# Reading the CSV file\n",
    "df_4= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5df950ee-eced-4e30-9c8c-c0129bc9b182",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data lengths: 213, 213, 213, 213\n",
      "Scraping Successfully Completed\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver_R = webdriver.Chrome()\n",
    "\n",
    "# Data storage lists\n",
    "Bus_names_r = []\n",
    "Bus_types_r = []\n",
    "Start_Time_r = []\n",
    "End_Time_r = []\n",
    "Ratings_r = []\n",
    "Total_Duration_r = []\n",
    "Prices_r = []\n",
    "Seats_Available_r = []\n",
    "Route_names = []\n",
    "Route_links = []\n",
    "\n",
    "# Loop through DataFrame rows\n",
    "for i, r in df_4.iterrows():\n",
    "    link = r[\"ROUTE_link\"]\n",
    "    routes = r[\"Route_name\"]\n",
    "\n",
    "    try:\n",
    "        # Open link\n",
    "        driver_R.get(link)\n",
    "        WebDriverWait(driver_R, 10).until(EC.presence_of_element_located((By.XPATH, \"//body\")))\n",
    "\n",
    "        # Click elements (if required)\n",
    "        elements = driver_R.find_elements(By.XPATH, f\"//a[contains(@href, '{link}')]\")\n",
    "        for element in elements:\n",
    "            element.click()\n",
    "            WebDriverWait(driver_R, 10).until(EC.presence_of_element_located((By.XPATH, \"//body\")))\n",
    "\n",
    "        # Scroll through the page\n",
    "        scrolling = True\n",
    "        while scrolling:\n",
    "            old_page_source = driver_R.page_source\n",
    "            ActionChains(driver_R).send_keys(Keys.PAGE_DOWN).perform()\n",
    "            time.sleep(2)  # Adjust as needed\n",
    "            new_page_source = driver_R.page_source\n",
    "            if new_page_source == old_page_source:\n",
    "                scrolling = False\n",
    "\n",
    "        # Extract data\n",
    "        bus_name = driver_R.find_elements(By.XPATH, \"//div[@class='travels lh-24 f-bold d-color']\")\n",
    "        bus_type = driver_R.find_elements(By.XPATH, \"//div[@class='bus-type f-12 m-top-16 l-color evBus']\")\n",
    "        start_time = driver_R.find_elements(By.XPATH, \"//*[@class='dp-time f-19 d-color f-bold']\")\n",
    "        end_time = driver_R.find_elements(By.XPATH, \"//*[@class='bp-time f-19 d-color disp-Inline']\")\n",
    "        total_duration = driver_R.find_elements(By.XPATH, \"//*[@class='dur l-color lh-24']\")\n",
    "        rating = driver_R.find_elements(By.XPATH, \"//div[@class='clearfix row-one']/div[@class='column-six p-right-10 w-10 fl']\")\n",
    "        price = driver_R.find_elements(By.XPATH, \"//*[@class='fare d-block']\")\n",
    "        seats = driver_R.find_elements(By.XPATH, \"//div[contains(@class, 'seat-left')]\")\n",
    "\n",
    "        # Append data to respective lists\n",
    "        for bus in bus_name:\n",
    "            Bus_names_r.append(bus.text)\n",
    "            Route_links.append(link)\n",
    "            Route_names.append(routes)\n",
    "        for bus_type_elem in bus_type:\n",
    "            Bus_types_r.append(bus_type_elem.text)\n",
    "        for start_time_elem in start_time:\n",
    "            Start_Time_r.append(start_time_elem.text)\n",
    "        for end_time_elem in end_time:\n",
    "            End_Time_r.append(end_time_elem.text)\n",
    "        for total_duration_elem in total_duration:\n",
    "            Total_Duration_r.append(total_duration_elem.text)\n",
    "        for rating_elem in rating:\n",
    "            Ratings_r.append(rating_elem.text)\n",
    "        for price_elem in price:\n",
    "            Prices_r.append(price_elem.text)\n",
    "        for seats_elem in seats:\n",
    "            Seats_Available_r.append(seats_elem.text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Check data consistency\n",
    "print(f\"Scraped data lengths: {len(Bus_names_r)}, {len(Bus_types_r)}, {len(Start_Time_r)}, {len(End_Time_r)}\")\n",
    "\n",
    "print(\"Scraping Successfully Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "525c7cc0-d94f-48dd-bb84-876679ac8509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of data lists: {'Bus_name': 213, 'Bus_type': 213, 'Start_time': 213, 'End_time': 213, 'Total_duration': 213, 'Price': 213, 'Seats_Available': 213, 'Ratings': 213, 'Route_link': 213, 'Route_name': 213}\n",
      "Data successfully saved to C:/Users/Sukkiiii/Desktop/CAPSTONE PROJECT/RED BUS/df_buses_5.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create DataFrame\n",
    "data_4= {\n",
    "    'Bus_name': Bus_names_r,\n",
    "    'Bus_type': Bus_types_r,\n",
    "    'Start_time': Start_Time_r,\n",
    "    'End_time': End_Time_r,\n",
    "    'Total_duration': Total_Duration_r,\n",
    "    'Price': Prices_r,\n",
    "    \"Seats_Available\": Seats_Available_r,\n",
    "    \"Ratings\": Ratings_r,\n",
    "    'Route_link': Route_links,\n",
    "    'Route_name': Route_names\n",
    "}\n",
    "\n",
    "# Validate lengths of lists\n",
    "lengths = {key: len(value) for key, value in data_4.items()}\n",
    "print(\"Lengths of data lists:\", lengths)\n",
    "\n",
    "# Check for inconsistencies\n",
    "if len(set(lengths.values())) != 1:\n",
    "    print(\"Error: Mismatch in lengths of data lists.\")\n",
    "else:\n",
    "    # Save to CSV\n",
    "    path = r\"C:/Users/Sukkiiii/Desktop/CAPSTONE PROJECT/RED BUS/df_buses_5.csv\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)  # Create directories if they don't exist\n",
    "    df_buses_5 = pd.DataFrame(data_4)\n",
    "    df_buses_5.to_csv(path, index=False)\n",
    "    print(f\"Data successfully saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc0a1621-04a4-4896-8938-270ca734e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\Sukkiiii\\Desktop\\CAPSTONE PROJECT\\RED BUS\\df_sb.csv\"\n",
    "\n",
    "# Reading the CSV file\n",
    "df_5= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "827f9478-6fa7-4edb-a991-9014e371d3b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data lengths: 66, 66, 66, 66\n",
      "Scraping Successfully Completed\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver_SB = webdriver.Chrome()\n",
    "\n",
    "# Data storage lists\n",
    "Bus_names_sb = []\n",
    "Bus_types_sb = []\n",
    "Start_Time_sb = []\n",
    "End_Time_sb = []\n",
    "Ratings_sb = []\n",
    "Total_Duration_sb = []\n",
    "Prices_sb = []\n",
    "Seats_Available_sb = []\n",
    "Route_names = []\n",
    "Route_links = []\n",
    "\n",
    "# Loop through DataFrame rows\n",
    "for i, r in df_5.iterrows():\n",
    "    link = r[\"ROUTE_link\"]\n",
    "    routes = r[\"Route_name\"]\n",
    "\n",
    "    try:\n",
    "        # Open link\n",
    "        driver_SB.get(link)\n",
    "        WebDriverWait(driver_SB, 10).until(EC.presence_of_element_located((By.XPATH, \"//body\")))\n",
    "\n",
    "        # Click elements (if required)\n",
    "        elements = driver_SB.find_elements(By.XPATH, f\"//a[contains(@href, '{link}')]\")\n",
    "        for element in elements:\n",
    "            element.click()\n",
    "            WebDriverWait(driver_SB, 10).until(EC.presence_of_element_located((By.XPATH, \"//body\")))\n",
    "\n",
    "        # Scroll through the page\n",
    "        scrolling = True\n",
    "        while scrolling:\n",
    "            old_page_source = driver_SB.page_source\n",
    "            ActionChains(driver_SB).send_keys(Keys.PAGE_DOWN).perform()\n",
    "            time.sleep(2)  # Adjust as needed\n",
    "            new_page_source = driver_SB.page_source\n",
    "            if new_page_source == old_page_source:\n",
    "                scrolling = False\n",
    "\n",
    "        # Extract data\n",
    "        bus_name = driver_SB.find_elements(By.XPATH, \"//div[@class='travels lh-24 f-bold d-color']\")\n",
    "        bus_type = driver_SB.find_elements(By.XPATH, \"//div[@class='bus-type f-12 m-top-16 l-color evBus']\")\n",
    "        start_time = driver_SB.find_elements(By.XPATH, \"//*[@class='dp-time f-19 d-color f-bold']\")\n",
    "        end_time = driver_SB.find_elements(By.XPATH, \"//*[@class='bp-time f-19 d-color disp-Inline']\")\n",
    "        total_duration = driver_SB.find_elements(By.XPATH, \"//*[@class='dur l-color lh-24']\")\n",
    "        rating = driver_SB.find_elements(By.XPATH, \"//div[@class='clearfix row-one']/div[@class='column-six p-right-10 w-10 fl']\")\n",
    "        price = driver_SB.find_elements(By.XPATH, \"//*[@class='fare d-block']\")\n",
    "        seats = driver_SB.find_elements(By.XPATH, \"//div[contains(@class, 'seat-left')]\")\n",
    "\n",
    "        # Append data to respective lists\n",
    "        for bus in bus_name:\n",
    "            Bus_names_sb.append(bus.text)\n",
    "            Route_links.append(link)\n",
    "            Route_names.append(routes)\n",
    "        for bus_type_elem in bus_type:\n",
    "            Bus_types_sb.append(bus_type_elem.text)\n",
    "        for start_time_elem in start_time:\n",
    "            Start_Time_sb.append(start_time_elem.text)\n",
    "        for end_time_elem in end_time:\n",
    "            End_Time_sb.append(end_time_elem.text)\n",
    "        for total_duration_elem in total_duration:\n",
    "            Total_Duration_sb.append(total_duration_elem.text)\n",
    "        for rating_elem in rating:\n",
    "            Ratings_sb.append(rating_elem.text)\n",
    "        for price_elem in price:\n",
    "            Prices_sb.append(price_elem.text)\n",
    "        for seats_elem in seats:\n",
    "            Seats_Available_sb.append(seats_elem.text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Check data consistency\n",
    "print(f\"Scraped data lengths: {len(Bus_names_sb)}, {len(Bus_types_sb)}, {len(Start_Time_sb)}, {len(End_Time_sb)}\")\n",
    "\n",
    "print(\"Scraping Successfully Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a7aef3f8-7e62-4ebc-86c7-804fadab6d66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of data lists: {'Bus_name': 66, 'Bus_type': 66, 'Start_time': 66, 'End_time': 66, 'Total_duration': 66, 'Price': 66, 'Seats_Available': 66, 'Ratings': 66, 'Route_link': 66, 'Route_name': 66}\n",
      "Data successfully saved to C:/Users/Sukkiiii/Desktop/CAPSTONE PROJECT/RED BUS/df_buses_6.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create DataFrame\n",
    "data_5= {\n",
    "    'Bus_name': Bus_names_sb,\n",
    "    'Bus_type': Bus_types_sb,\n",
    "    'Start_time': Start_Time_sb,\n",
    "    'End_time': End_Time_sb,\n",
    "    'Total_duration': Total_Duration_sb,\n",
    "    'Price': Prices_sb,\n",
    "    \"Seats_Available\": Seats_Available_sb,\n",
    "    \"Ratings\": Ratings_sb,\n",
    "    'Route_link': Route_links,\n",
    "    'Route_name': Route_names\n",
    "}\n",
    "\n",
    "# Validate lengths of lists\n",
    "lengths = {key: len(value) for key, value in data_5.items()}\n",
    "print(\"Lengths of data lists:\", lengths)\n",
    "\n",
    "# Check for inconsistencies\n",
    "if len(set(lengths.values())) != 1:\n",
    "    print(\"Error: Mismatch in lengths of data lists.\")\n",
    "else:\n",
    "    # Save to CSV\n",
    "    path = r\"C:/Users/Sukkiiii/Desktop/CAPSTONE PROJECT/RED BUS/df_buses_6.csv\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)  # Create directories if they don't exist\n",
    "    df_buses_6 = pd.DataFrame(data_5)\n",
    "    df_buses_6.to_csv(path, index=False)\n",
    "    print(f\"Data successfully saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ef09d10f-51e5-41bb-8c81-238724ce6d69",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\Sukkiiii\\Desktop\\CAPSTONE PROJECT\\RED BUS\\df_h.csv\"\n",
    "\n",
    "# Reading the CSV file\n",
    "df_6= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "06f8572f-9cb0-493d-a11c-1cdfb08123f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data lengths: 218, 218, 218, 218\n",
      "Scraping Successfully Completed\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver_H = webdriver.Chrome()\n",
    "\n",
    "# Data storage lists\n",
    "Bus_names_h = []\n",
    "Bus_types_h = []\n",
    "Start_Time_h = []\n",
    "End_Time_h = []\n",
    "Ratings_h = []\n",
    "Total_Duration_h = []\n",
    "Prices_h = []\n",
    "Seats_Available_h = []\n",
    "Route_names = []\n",
    "Route_links = []\n",
    "\n",
    "# Loop through DataFrame rows\n",
    "for i, r in df_6.iterrows():\n",
    "    link = r[\"ROUTE_link\"]\n",
    "    routes = r[\"Route_name\"]\n",
    "\n",
    "    try:\n",
    "        # Open link\n",
    "        driver_H.get(link)\n",
    "        WebDriverWait(driver_H, 10).until(EC.presence_of_element_located((By.XPATH, \"//body\")))\n",
    "\n",
    "        # Click elements (if required)\n",
    "        elements = driver_H.find_elements(By.XPATH, f\"//a[contains(@href, '{link}')]\")\n",
    "        for element in elements:\n",
    "            element.click()\n",
    "            WebDriverWait(driver_H, 10).until(EC.presence_of_element_located((By.XPATH, \"//body\")))\n",
    "\n",
    "        # Scroll through the page\n",
    "        scrolling = True\n",
    "        while scrolling:\n",
    "            old_page_source = driver_H.page_source\n",
    "            ActionChains(driver_H).send_keys(Keys.PAGE_DOWN).perform()\n",
    "            time.sleep(2)  # Adjust as needed\n",
    "            new_page_source = driver_H.page_source\n",
    "            if new_page_source == old_page_source:\n",
    "                scrolling = False\n",
    "\n",
    "        # Extract data\n",
    "        bus_name = driver_H.find_elements(By.XPATH, \"//div[@class='travels lh-24 f-bold d-color']\")\n",
    "        bus_type = driver_H.find_elements(By.XPATH, \"//div[@class='bus-type f-12 m-top-16 l-color evBus']\")\n",
    "        start_time = driver_H.find_elements(By.XPATH, \"//*[@class='dp-time f-19 d-color f-bold']\")\n",
    "        end_time = driver_H.find_elements(By.XPATH, \"//*[@class='bp-time f-19 d-color disp-Inline']\")\n",
    "        total_duration = driver_H.find_elements(By.XPATH, \"//*[@class='dur l-color lh-24']\")\n",
    "        rating = driver_H.find_elements(By.XPATH, \"//div[@class='clearfix row-one']/div[@class='column-six p-right-10 w-10 fl']\")\n",
    "        price = driver_H.find_elements(By.XPATH, \"//*[@class='fare d-block']\")\n",
    "        seats = driver_H.find_elements(By.XPATH, \"//div[contains(@class, 'seat-left')]\")\n",
    "\n",
    "        # Append data to respective lists\n",
    "        for bus in bus_name:\n",
    "            Bus_names_h.append(bus.text)\n",
    "            Route_links.append(link)\n",
    "            Route_names.append(routes)\n",
    "        for bus_type_elem in bus_type:\n",
    "            Bus_types_h.append(bus_type_elem.text)\n",
    "        for start_time_elem in start_time:\n",
    "            Start_Time_h.append(start_time_elem.text)\n",
    "        for end_time_elem in end_time:\n",
    "            End_Time_h.append(end_time_elem.text)\n",
    "        for total_duration_elem in total_duration:\n",
    "            Total_Duration_h.append(total_duration_elem.text)\n",
    "        for rating_elem in rating:\n",
    "            Ratings_h.append(rating_elem.text)\n",
    "        for price_elem in price:\n",
    "            Prices_h.append(price_elem.text)\n",
    "        for seats_elem in seats:\n",
    "            Seats_Available_h.append(seats_elem.text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Check data consistency\n",
    "print(f\"Scraped data lengths: {len(Bus_names_h)}, {len(Bus_types_h)}, {len(Start_Time_h)}, {len(End_Time_h)}\")\n",
    "\n",
    "print(\"Scraping Successfully Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9a610edd-481c-4dab-b9db-72b8f453d5db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of data lists: {'Bus_name': 218, 'Bus_type': 218, 'Start_time': 218, 'End_time': 218, 'Total_duration': 218, 'Price': 218, 'Seats_Available': 218, 'Ratings': 218, 'Route_link': 218, 'Route_name': 218}\n",
      "Data successfully saved to C:/Users/Sukkiiii/Desktop/CAPSTONE PROJECT/RED BUS/df_buses_7.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create DataFrame\n",
    "data_6= {\n",
    "    'Bus_name': Bus_names_h,\n",
    "    'Bus_type': Bus_types_h,\n",
    "    'Start_time': Start_Time_h,\n",
    "    'End_time': End_Time_h,\n",
    "    'Total_duration': Total_Duration_h,\n",
    "    'Price': Prices_h,\n",
    "    \"Seats_Available\": Seats_Available_h,\n",
    "    \"Ratings\": Ratings_h,\n",
    "    'Route_link': Route_links,\n",
    "    'Route_name': Route_names\n",
    "}\n",
    "\n",
    "# Validate lengths of lists\n",
    "lengths = {key: len(value) for key, value in data_6.items()}\n",
    "print(\"Lengths of data lists:\", lengths)\n",
    "\n",
    "# Check for inconsistencies\n",
    "if len(set(lengths.values())) != 1:\n",
    "    print(\"Error: Mismatch in lengths of data lists.\")\n",
    "else:\n",
    "    # Save to CSV\n",
    "    path = r\"C:/Users/Sukkiiii/Desktop/CAPSTONE PROJECT/RED BUS/df_buses_7.csv\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)  # Create directories if they don't exist\n",
    "    df_buses_7 = pd.DataFrame(data_6)\n",
    "    df_buses_7.to_csv(path, index=False)\n",
    "    print(f\"Data successfully saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a59fec27-31b6-4dbf-b896-1910db4d3417",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\Sukkiiii\\Desktop\\CAPSTONE PROJECT\\RED BUS\\df_ar.csv\"\n",
    "\n",
    "# Reading the CSV file\n",
    "df_7= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "c9b04fa6-ac53-495a-8ba9-10ae13a70969",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data lengths: 104, 104, 104, 104\n",
      "Scraping Successfully Completed\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver_AR = webdriver.Chrome()\n",
    "\n",
    "# Data storage lists\n",
    "Bus_names_ar = []\n",
    "Bus_types_ar = []\n",
    "Start_Time_ar = []\n",
    "End_Time_ar = []\n",
    "Ratings_ar = []\n",
    "Total_Duration_ar = []\n",
    "Prices_ar = []\n",
    "Seats_Available_ar = []\n",
    "Route_names = []\n",
    "Route_links = []\n",
    "\n",
    "# Loop through DataFrame rows\n",
    "for i, r in df_7.iterrows():\n",
    "    link = r[\"ROUTE_link\"]\n",
    "    routes = r[\"Route_name\"]\n",
    "\n",
    "    try:\n",
    "        # Open link\n",
    "        driver_AR.get(link)\n",
    "        WebDriverWait(driver_AR, 10).until(EC.presence_of_element_located((By.XPATH, \"//body\")))\n",
    "\n",
    "        # Click elements (if required)\n",
    "        elements = driver_AR.find_elements(By.XPATH, f\"//a[contains(@href, '{link}')]\")\n",
    "        for element in elements:\n",
    "            element.click()\n",
    "            WebDriverWait(driver_AR, 10).until(EC.presence_of_element_located((By.XPATH, \"//body\")))\n",
    "\n",
    "        # Scroll through the page\n",
    "        scrolling = True\n",
    "        while scrolling:\n",
    "            old_page_source = driver_AR.page_source\n",
    "            ActionChains(driver_AR).send_keys(Keys.PAGE_DOWN).perform()\n",
    "            time.sleep(2)  # Adjust as needed\n",
    "            new_page_source = driver_AR.page_source\n",
    "            if new_page_source == old_page_source:\n",
    "                scrolling = False\n",
    "\n",
    "        # Extract data\n",
    "        bus_name = driver_AR.find_elements(By.XPATH, \"//div[@class='travels lh-24 f-bold d-color']\")\n",
    "        bus_type = driver_AR.find_elements(By.XPATH, \"//div[@class='bus-type f-12 m-top-16 l-color evBus']\")\n",
    "        start_time = driver_AR.find_elements(By.XPATH, \"//*[@class='dp-time f-19 d-color f-bold']\")\n",
    "        end_time = driver_AR.find_elements(By.XPATH, \"//*[@class='bp-time f-19 d-color disp-Inline']\")\n",
    "        total_duration = driver_AR.find_elements(By.XPATH, \"//*[@class='dur l-color lh-24']\")\n",
    "        rating = driver_AR.find_elements(By.XPATH, \"//div[@class='clearfix row-one']/div[@class='column-six p-right-10 w-10 fl']\")\n",
    "        price = driver_AR.find_elements(By.XPATH, \"//*[@class='fare d-block']\")\n",
    "        seats = driver_AR.find_elements(By.XPATH, \"//div[contains(@class, 'seat-left')]\")\n",
    "\n",
    "        # Append data to respective lists\n",
    "        for bus in bus_name:\n",
    "            Bus_names_ar.append(bus.text)\n",
    "            Route_links.append(link)\n",
    "            Route_names.append(routes)\n",
    "        for bus_type_elem in bus_type:\n",
    "            Bus_types_ar.append(bus_type_elem.text)\n",
    "        for start_time_elem in start_time:\n",
    "            Start_Time_ar.append(start_time_elem.text)\n",
    "        for end_time_elem in end_time:\n",
    "            End_Time_ar.append(end_time_elem.text)\n",
    "        for total_duration_elem in total_duration:\n",
    "            Total_Duration_ar.append(total_duration_elem.text)\n",
    "        for rating_elem in rating:\n",
    "            Ratings_ar.append(rating_elem.text)\n",
    "        for price_elem in price:\n",
    "            Prices_ar.append(price_elem.text)\n",
    "        for seats_elem in seats:\n",
    "            Seats_Available_ar.append(seats_elem.text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Check data consistency\n",
    "print(f\"Scraped data lengths: {len(Bus_names_ar)}, {len(Bus_types_ar)}, {len(Start_Time_ar)}, {len(End_Time_ar)}\")\n",
    "\n",
    "print(\"Scraping Successfully Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a66b07e2-e75c-4473-b75b-4ad24444c0d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of data lists: {'Bus_name': 104, 'Bus_type': 104, 'Start_time': 104, 'End_time': 104, 'Total_duration': 104, 'Price': 104, 'Seats_Available': 104, 'Ratings': 104, 'Route_link': 104, 'Route_name': 104}\n",
      "Data successfully saved to C:/Users/Sukkiiii/Desktop/CAPSTONE PROJECT/RED BUS/df_buses_8.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create DataFrame\n",
    "data_7= {\n",
    "    'Bus_name': Bus_names_ar,\n",
    "    'Bus_type': Bus_types_ar,\n",
    "    'Start_time': Start_Time_ar,\n",
    "    'End_time': End_Time_ar,\n",
    "    'Total_duration': Total_Duration_ar,\n",
    "    'Price': Prices_ar,\n",
    "    \"Seats_Available\": Seats_Available_ar,\n",
    "    \"Ratings\": Ratings_ar,\n",
    "    'Route_link': Route_links,\n",
    "    'Route_name': Route_names\n",
    "}\n",
    "\n",
    "# Validate lengths of lists\n",
    "lengths = {key: len(value) for key, value in data_7.items()}\n",
    "print(\"Lengths of data lists:\", lengths)\n",
    "\n",
    "# Check for inconsistencies\n",
    "if len(set(lengths.values())) != 1:\n",
    "    print(\"Error: Mismatch in lengths of data lists.\")\n",
    "else:\n",
    "    # Save to CSV\n",
    "    path = r\"C:/Users/Sukkiiii/Desktop/CAPSTONE PROJECT/RED BUS/df_buses_8.csv\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)  # Create directories if they don't exist\n",
    "    df_buses_8 = pd.DataFrame(data_7)\n",
    "    df_buses_8.to_csv(path, index=False)\n",
    "    print(f\"Data successfully saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2c752376-6a73-4bbb-b11e-4a72cf615cba",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\Sukkiiii\\Desktop\\CAPSTONE PROJECT\\RED BUS\\df_wb.csv\"\n",
    "\n",
    "# Reading the CSV file\n",
    "df_8= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "b8bc4e95-861d-4c1a-af48-2a9576b2a3ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data lengths: 43, 43, 43, 43\n",
      "Scraping Successfully Completed\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver_WB = webdriver.Chrome()\n",
    "\n",
    "# Data storage lists\n",
    "Bus_names_w = []\n",
    "Bus_types_w = []\n",
    "Start_Time_w = []\n",
    "End_Time_w = []\n",
    "Ratings_w = []\n",
    "Total_Duration_w = []\n",
    "Prices_w = []\n",
    "Seats_Available_w = []\n",
    "Route_names = []\n",
    "Route_links = []\n",
    "\n",
    "# Loop through DataFrame rows\n",
    "for i, r in df_8.iterrows():\n",
    "    link = r[\"ROUTE_link\"]\n",
    "    routes = r[\"Route_name\"]\n",
    "\n",
    "    try:\n",
    "        # Open link\n",
    "        driver_WB.get(link)\n",
    "        WebDriverWait(driver_WB, 10).until(EC.presence_of_element_located((By.XPATH, \"//body\")))\n",
    "\n",
    "        # Click elements (if required)\n",
    "        elements = driver_WB.find_elements(By.XPATH, f\"//a[contains(@href, '{link}')]\")\n",
    "        for element in elements:\n",
    "            element.click()\n",
    "            WebDriverWait(driver_WB, 10).until(EC.presence_of_element_located((By.XPATH, \"//body\")))\n",
    "\n",
    "        # Scroll through the page\n",
    "        scrolling = True\n",
    "        while scrolling:\n",
    "            old_page_source = driver_WB.page_source\n",
    "            ActionChains(driver_WB).send_keys(Keys.PAGE_DOWN).perform()\n",
    "            time.sleep(2)  # Adjust as needed\n",
    "            new_page_source = driver_WB.page_source\n",
    "            if new_page_source == old_page_source:\n",
    "                scrolling = False\n",
    "\n",
    "        # Extract data\n",
    "        bus_name = driver_WB.find_elements(By.XPATH, \"//div[@class='travels lh-24 f-bold d-color']\")\n",
    "        bus_type = driver_WB.find_elements(By.XPATH, \"//div[@class='bus-type f-12 m-top-16 l-color evBus']\")\n",
    "        start_time = driver_WB.find_elements(By.XPATH, \"//*[@class='dp-time f-19 d-color f-bold']\")\n",
    "        end_time = driver_WB.find_elements(By.XPATH, \"//*[@class='bp-time f-19 d-color disp-Inline']\")\n",
    "        total_duration = driver_WB.find_elements(By.XPATH, \"//*[@class='dur l-color lh-24']\")\n",
    "        rating = driver_WB.find_elements(By.XPATH, \"//div[@class='clearfix row-one']/div[@class='column-six p-right-10 w-10 fl']\")\n",
    "        price = driver_WB.find_elements(By.XPATH, \"//*[@class='fare d-block']\")\n",
    "        seats = driver_WB.find_elements(By.XPATH, \"//div[contains(@class, 'seat-left')]\")\n",
    "\n",
    "        # Append data to respective lists\n",
    "        for bus in bus_name:\n",
    "            Bus_names_w.append(bus.text)\n",
    "            Route_links.append(link)\n",
    "            Route_names.append(routes)\n",
    "        for bus_type_elem in bus_type:\n",
    "            Bus_types_w.append(bus_type_elem.text)\n",
    "        for start_time_elem in start_time:\n",
    "            Start_Time_w.append(start_time_elem.text)\n",
    "        for end_time_elem in end_time:\n",
    "            End_Time_w.append(end_time_elem.text)\n",
    "        for total_duration_elem in total_duration:\n",
    "            Total_Duration_w.append(total_duration_elem.text)\n",
    "        for rating_elem in rating:\n",
    "            Ratings_w.append(rating_elem.text)\n",
    "        for price_elem in price:\n",
    "            Prices_w.append(price_elem.text)\n",
    "        for seats_elem in seats:\n",
    "            Seats_Available_w.append(seats_elem.text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Check data consistency\n",
    "print(f\"Scraped data lengths: {len(Bus_names_w)}, {len(Bus_types_w)}, {len(Start_Time_w)}, {len(End_Time_w)}\")\n",
    "\n",
    "print(\"Scraping Successfully Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c93fd38e-b3bb-42df-8dc5-1cc3ffcc67f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of data lists: {'Bus_name': 43, 'Bus_type': 43, 'Start_time': 43, 'End_time': 43, 'Total_duration': 43, 'Price': 43, 'Seats_Available': 43, 'Ratings': 43, 'Route_link': 43, 'Route_name': 43}\n",
      "Data successfully saved to C:/Users/Sukkiiii/Desktop/CAPSTONE PROJECT/RED BUS/df_buses_9.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create DataFrame\n",
    "data_8= {\n",
    "    'Bus_name': Bus_names_w,\n",
    "    'Bus_type': Bus_types_w,\n",
    "    'Start_time': Start_Time_w,\n",
    "    'End_time': End_Time_w,\n",
    "    'Total_duration': Total_Duration_w,\n",
    "    'Price': Prices_w,\n",
    "    \"Seats_Available\": Seats_Available_w,\n",
    "    \"Ratings\": Ratings_w,\n",
    "    'Route_link': Route_links,\n",
    "    'Route_name': Route_names\n",
    "}\n",
    "\n",
    "# Validate lengths of lists\n",
    "lengths = {key: len(value) for key, value in data_8.items()}\n",
    "print(\"Lengths of data lists:\", lengths)\n",
    "\n",
    "# Check for inconsistencies\n",
    "if len(set(lengths.values())) != 1:\n",
    "    print(\"Error: Mismatch in lengths of data lists.\")\n",
    "else:\n",
    "    # Save to CSV\n",
    "    path = r\"C:/Users/Sukkiiii/Desktop/CAPSTONE PROJECT/RED BUS/df_buses_9.csv\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)  # Create directories if they don't exist\n",
    "    df_buses_9 = pd.DataFrame(data_8)\n",
    "    df_buses_9.to_csv(path, index=False)\n",
    "    print(f\"Data successfully saved to {path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "0c44df66-2c4c-43d8-b233-265717388847",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = r\"C:\\Users\\Sukkiiii\\Desktop\\CAPSTONE PROJECT\\RED BUS\\df_up.csv\"\n",
    "\n",
    "# Reading the CSV file\n",
    "df_9= pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9b5e0334-c702-42a1-a336-dc2b2040a564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraped data lengths: 172, 172, 172, 172\n",
      "Scraping Successfully Completed\n"
     ]
    }
   ],
   "source": [
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "# Initialize WebDriver\n",
    "driver_U = webdriver.Chrome()\n",
    "\n",
    "# Data storage lists\n",
    "Bus_names_u = []\n",
    "Bus_types_u = []\n",
    "Start_Time_u = []\n",
    "End_Time_u = []\n",
    "Ratings_u = []\n",
    "Total_Duration_u = []\n",
    "Prices_u = []\n",
    "Seats_Available_u = []\n",
    "Route_names = []\n",
    "Route_links = []\n",
    "\n",
    "# Loop through DataFrame rows\n",
    "for i, r in df_9.iterrows():\n",
    "    link = r[\"ROUTE_link\"]\n",
    "    routes = r[\"Route_name\"]\n",
    "\n",
    "    try:\n",
    "        # Open link\n",
    "        driver_U.get(link)\n",
    "        WebDriverWait(driver_U, 10).until(EC.presence_of_element_located((By.XPATH, \"//body\")))\n",
    "\n",
    "        # Click elements (if required)\n",
    "        elements = driver_U.find_elements(By.XPATH, f\"//a[contains(@href, '{link}')]\")\n",
    "        for element in elements:\n",
    "            element.click()\n",
    "            WebDriverWait(driver_U, 10).until(EC.presence_of_element_located((By.XPATH, \"//body\")))\n",
    "\n",
    "        # Scroll through the page\n",
    "        scrolling = True\n",
    "        while scrolling:\n",
    "            old_page_source = driver_U.page_source\n",
    "            ActionChains(driver_U).send_keys(Keys.PAGE_DOWN).perform()\n",
    "            time.sleep(2)  # Adjust as needed\n",
    "            new_page_source = driver_U.page_source\n",
    "            if new_page_source == old_page_source:\n",
    "                scrolling = False\n",
    "\n",
    "        # Extract data\n",
    "        bus_name = driver_U.find_elements(By.XPATH, \"//div[@class='travels lh-24 f-bold d-color']\")\n",
    "        bus_type = driver_U.find_elements(By.XPATH, \"//div[@class='bus-type f-12 m-top-16 l-color evBus']\")\n",
    "        start_time = driver_U.find_elements(By.XPATH, \"//*[@class='dp-time f-19 d-color f-bold']\")\n",
    "        end_time = driver_U.find_elements(By.XPATH, \"//*[@class='bp-time f-19 d-color disp-Inline']\")\n",
    "        total_duration = driver_U.find_elements(By.XPATH, \"//*[@class='dur l-color lh-24']\")\n",
    "        rating = driver_U.find_elements(By.XPATH, \"//div[@class='clearfix row-one']/div[@class='column-six p-right-10 w-10 fl']\")\n",
    "        price = driver_U.find_elements(By.XPATH, \"//*[@class='fare d-block']\")\n",
    "        seats = driver_U.find_elements(By.XPATH, \"//div[contains(@class, 'seat-left')]\")\n",
    "\n",
    "        # Append data to respective lists\n",
    "        for bus in bus_name:\n",
    "            Bus_names_u.append(bus.text)\n",
    "            Route_links.append(link)\n",
    "            Route_names.append(routes)\n",
    "        for bus_type_elem in bus_type:\n",
    "            Bus_types_u.append(bus_type_elem.text)\n",
    "        for start_time_elem in start_time:\n",
    "            Start_Time_u.append(start_time_elem.text)\n",
    "        for end_time_elem in end_time:\n",
    "            End_Time_u.append(end_time_elem.text)\n",
    "        for total_duration_elem in total_duration:\n",
    "            Total_Duration_u.append(total_duration_elem.text)\n",
    "        for rating_elem in rating:\n",
    "            Ratings_u.append(rating_elem.text)\n",
    "        for price_elem in price:\n",
    "            Prices_u.append(price_elem.text)\n",
    "        for seats_elem in seats:\n",
    "            Seats_Available_u.append(seats_elem.text)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing row {i}: {e}\")\n",
    "        continue\n",
    "\n",
    "# Check data consistency\n",
    "print(f\"Scraped data lengths: {len(Bus_names_u)}, {len(Bus_types_u)}, {len(Start_Time_u)}, {len(End_Time_u)}\")\n",
    "\n",
    "print(\"Scraping Successfully Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "74c77b0c-6c8e-4910-af1e-012fecf62696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lengths of data lists: {'Bus_name': 172, 'Bus_type': 172, 'Start_time': 172, 'End_time': 172, 'Total_duration': 172, 'Price': 172, 'Seats_Available': 172, 'Ratings': 172, 'Route_link': 172, 'Route_name': 172}\n",
      "Data successfully saved to C:/Users/Sukkiiii/Desktop/CAPSTONE PROJECT/RED BUS/df_buses_10.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Create DataFrame\n",
    "data_9= {\n",
    "    'Bus_name': Bus_names_u,\n",
    "    'Bus_type': Bus_types_u,\n",
    "    'Start_time': Start_Time_u,\n",
    "    'End_time': End_Time_u,\n",
    "    'Total_duration': Total_Duration_u,\n",
    "    'Price': Prices_u,\n",
    "    \"Seats_Available\": Seats_Available_u,\n",
    "    \"Ratings\": Ratings_u,\n",
    "    'Route_link': Route_links,\n",
    "    'Route_name': Route_names\n",
    "}\n",
    "\n",
    "# Validate lengths of lists\n",
    "lengths = {key: len(value) for key, value in data_9.items()}\n",
    "print(\"Lengths of data lists:\", lengths)\n",
    "\n",
    "# Check for inconsistencies\n",
    "if len(set(lengths.values())) != 1:\n",
    "    print(\"Error: Mismatch in lengths of data lists.\")\n",
    "else:\n",
    "    # Save to CSV\n",
    "    path = r\"C:/Users/Sukkiiii/Desktop/CAPSTONE PROJECT/RED BUS/df_buses_10.csv\"\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)  # Create directories if they don't exist\n",
    "    df_buses_10 = pd.DataFrame(data_9)\n",
    "    df_buses_10.to_csv(path, index=False)\n",
    "    print(f\"Data successfully saved to {path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
